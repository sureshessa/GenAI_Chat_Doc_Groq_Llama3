{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This code enables a chatbot interface that allows users to upload a PDF document and interact with it using the LLaMA3 language model. Users can ask questions related to the uploaded PDF, and the system processes the document by chunking its contents, storing them in a FAISS vector database, and retrieving relevant information to generate responses. The chatbot utilizes the LLaMA3 model to deliver accurate, document-based answers, ensuring that the responses are based solely on the content of the uploaded file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from llama_parse import LlamaParse \n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "import nest_asyncio  # noqa: E402\n",
    "from chromadb import Client\n",
    "from chromadb.config import Settings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss \n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models, particularly those designed for natural language processing (NLP) tasks, have become increasingly important in recent years due to their ability to process and generate human-like text at an unprecedented scale and speed. Here are some reasons why:\n",
      "\n",
      "1. **Efficient Language Understanding**: Fast language models can quickly analyze vast amounts of text data, enabling them to extract insights, identify patterns, and make predictions with high accuracy. This is essential for tasks like sentiment analysis, topic modeling, and text classification.\n",
      "2. **Scalability**: As data volumes continue to grow, fast language models can efficiently handle large amounts of text data, making them suitable for applications like information retrieval, language translation, and question-answering systems.\n",
      "3. **Real-time Processing**: Fast language models enable real-time processing of text data, which is crucial for applications like chatbots, voice assistants, and live customer support. They can respond promptly to user queries, providing a better user experience.\n",
      "4. **Improved User Experience**: By generating human-like text at high speeds, fast language models can enhance user experiences in various applications, such as:\n",
      "\t* Auto-complete and suggestion systems: Fast language models can provide relevant suggestions as users type, making it easier to find information.\n",
      "\t* Content generation: Fast language models can quickly generate text for applications like news articles, social media posts, and product descriptions.\n",
      "\t* Conversational interfaces: Fast language models can enable more natural and engaging conversations with users.\n",
      "5. **Advancements in AI**: Fast language models are essential for advancements in AI research, enabling the development of more sophisticated NLP systems, such as:\n",
      "\t* Neural language processing: Fast language models can be used to train neural networks for NLP tasks, leading to improved performance and accuracy.\n",
      "\t* Multimodal learning: Fast language models can be integrated with computer vision and audio processing to enable multimodal understanding.\n",
      "6. **Domain Adaptation**: Fast language models can quickly adapt to new domains or topics, which is crucial for applications like:\n",
      "\t* Sentiment analysis in specific industries (e.g., healthcare, finance)\n",
      "\t* Language translation for rare or obscure languages\n",
      "7. **Cost-Effective**: Fast language models can reduce computational costs and energy consumption, making them more efficient and cost-effective compared to traditional NLP approaches.\n",
      "\n",
      "Some notable examples of fast language models include:\n",
      "\n",
      "* Transformer-based models like BERT, RoBERTa, and DistilBERT, which have achieved state-of-the-art results in various NLP tasks.\n",
      "* Word-level models like Word2Vec and GloVe, which can generate word embeddings at high speeds.\n",
      "* Character-level models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, which can process characters at rapid speeds.\n",
      "\n",
      "In summary, fast language models have revolutionized NLP by enabling efficient, scalable, and accurate language understanding and processing. Their applications are vast, and they will continue to play a crucial role in shaping the future of AI and natural language processing.\n"
     ]
    }
   ],
   "source": [
    "# Access the llm from Groq\n",
    "client = Groq(\n",
    "    api_key=os.getenv(\"Groq_API_Key\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = LlamaParse(\n",
    "    api_key=os.getenv(\"LlamaIndex_API_Key\"),\n",
    "    result_type=\"markdown\"  # \"markdown\" and \"text\" are available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 3635970e-e7a8-4c09-a63b-dcfefb6fdd49\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()\n",
    "file_extractor = {\".pdf\": parser}\n",
    "documents = SimpleDirectoryReader(input_files=['data/Rainwater_storage.pdf'], file_extractor=file_extractor).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_with_splitter(text, chunk_size=1000, chunk_overlap=100):\n",
    "    \"\"\"Split the text into smaller chunks using RecursiveCharacterTextSplitter.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FAISS Index\n",
    "def create_faiss_index(dim):\n",
    "    \"\"\"Create a FAISS index for the given dimension.\"\"\"\n",
    "    index = faiss.IndexFlatL2(dim)  # Using L2 distance\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/genai/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize SentenceTransformer for embedding\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to store chunks into a pickle file\n",
    "def store_chunks_to_pickle(chunks, file_name='text_chunks.pkl'):\n",
    "    \"\"\"Append chunks to a pickle file.\"\"\"\n",
    "    try:\n",
    "        # Load existing chunks if file exists\n",
    "        with open(file_name, 'rb') as file:\n",
    "            existing_chunks = pickle.load(file)\n",
    "    except FileNotFoundError:\n",
    "        # If file doesn't exist, start with an empty list\n",
    "        existing_chunks = []\n",
    "\n",
    "    # Append new chunks to existing ones\n",
    "    existing_chunks.extend(chunks)\n",
    "\n",
    "    # Store the updated chunks back to the pickle file\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(existing_chunks, file)\n",
    "\n",
    "    print(f\"Stored {len(chunks)} new chunks into {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(text):\n",
    "    \"\"\"Convert text to embeddings.\"\"\"\n",
    "    return model.encode(text).astype('float32')  # Ensure embeddings are in float32 format\n",
    "\n",
    "# Step 6: Store embeddings in FAISS\n",
    "def store_embeddings(index, chunks):\n",
    "    \"\"\"Store embeddings in FAISS index.\"\"\"\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        chunk_embedding = embed_text(chunk)\n",
    "        embeddings.append(chunk_embedding)\n",
    "\n",
    "    # Convert to a numpy array and add to the FAISS index\n",
    "    embeddings_np = np.array(embeddings).astype('float32')\n",
    "    index.add(embeddings_np)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 3 new chunks into text_chunks.pkl\n",
      "Processed Document 1 with 3 chunks.\n",
      "Stored 4 new chunks into text_chunks.pkl\n",
      "Processed Document 2 with 4 chunks.\n",
      "Stored 4 new chunks into text_chunks.pkl\n",
      "Processed Document 3 with 4 chunks.\n",
      "Stored 2 new chunks into text_chunks.pkl\n",
      "Processed Document 4 with 2 chunks.\n",
      "Stored 2 new chunks into text_chunks.pkl\n",
      "Processed Document 5 with 2 chunks.\n",
      "Stored 2 new chunks into text_chunks.pkl\n",
      "Processed Document 6 with 2 chunks.\n",
      "Stored 4 new chunks into text_chunks.pkl\n",
      "Processed Document 7 with 4 chunks.\n",
      "Stored 1 new chunks into text_chunks.pkl\n",
      "Processed Document 8 with 1 chunks.\n",
      "Stored 4 new chunks into text_chunks.pkl\n",
      "Processed Document 9 with 4 chunks.\n",
      "Stored 1 new chunks into text_chunks.pkl\n",
      "Processed Document 10 with 1 chunks.\n",
      "Stored 4 new chunks into text_chunks.pkl\n",
      "Processed Document 11 with 4 chunks.\n",
      "Stored 3 new chunks into text_chunks.pkl\n",
      "Processed Document 12 with 3 chunks.\n",
      "Stored 1 new chunks into text_chunks.pkl\n",
      "Processed Document 13 with 1 chunks.\n",
      "Stored embeddings into FAISS index and saved to 'faiss_index.index'.\n",
      "Total number of embeddings stored: 35\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Process all documents and store their chunks in FAISS\n",
    "faiss_index = create_faiss_index(dim=384)  # Set dimension to match your model's output size\n",
    "\n",
    "for doc_idx, document in enumerate(documents):\n",
    "    # Extract text from each document\n",
    "    doc_text = getattr(document, 'text', \"\")\n",
    "    \n",
    "    if doc_text:  # Only process if the document has text\n",
    "        # Split the document into chunks\n",
    "        chunks = chunk_text_with_splitter(doc_text, chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "        # Store chunks in pickle file\n",
    "        store_chunks_to_pickle(chunks, file_name='text_chunks.pkl')\n",
    "\n",
    "        # Store chunks in FAISS\n",
    "        faiss_index = store_embeddings(faiss_index, chunks)\n",
    "\n",
    "        print(f\"Processed Document {doc_idx + 1} with {len(chunks)} chunks.\")\n",
    "    else:\n",
    "        print(f\"Skipping empty document {doc_idx + 1}\")\n",
    "\n",
    "# Step 8: Save the FAISS index to disk\n",
    "faiss.write_index(faiss_index, 'faiss_index.index')\n",
    "print(\"Stored embeddings into FAISS index and saved to 'faiss_index.index'.\")\n",
    "\n",
    "# Optional: Load FAISS index from disk\n",
    "# faiss_index = faiss.read_index('faiss_index.index')\n",
    "\n",
    "# Step 9: Display the number of stored embeddings\n",
    "print(f\"Total number of embeddings stored: {faiss_index.ntotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Load chunks from pickle (for future use)\n",
    "def load_chunks_from_pickle(file_name='text_chunks.pkl'):\n",
    "    with open(file_name, 'rb') as file:\n",
    "        chunks = pickle.load(file)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(query, top_k=5):\n",
    "    \"\"\"Retrieve relevant information from FAISS and get LLM response.\"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = embed_text(query).reshape(1, -1)\n",
    "    # Load the FAISS index from file\n",
    "    faiss_index = faiss.read_index('faiss_index.index')\n",
    "\n",
    "    # Retrieve the top_k nearest neighbors\n",
    "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
    "    chunks = load_chunks_from_pickle()\n",
    "    # Get the corresponding chunks from the index\n",
    "    retrieved_chunks = []\n",
    "    for idx in indices[0]:\n",
    "        if idx >= 0:  # Ensure the index is valid\n",
    "            retrieved_chunks.append(chunks[idx])  # You need to maintain a separate list of chunks\n",
    "\n",
    "    # Construct the prompt for the LLM\n",
    "    prompt = f\"Based on the following information, {query}\\n\\n\" + \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "    # Use the Groq API to get the LLM response\n",
    "    client = Groq(api_key=os.getenv(\"Groq_API_Key\"))  # Replace with your actual Groq API key\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama3-8b-8192\",\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a possible introduction for \"Modeling Rainwater Harvesting Systems with Covered Storage Tank on A Smartphone\":\n",
      "\n",
      "Rainwater harvesting and storage have become increasingly relevant in today's water-scarce world. With the increasing frequency of droughts and water scarcity, finding innovative solutions to meet our water needs is more crucial than ever. One such approach is the rainwater harvesting system with a covered storage tank, which offers a simple and energy-efficient way to collect and store rainwater for future use. This type of system, commonly referred to as RWHS, has gained popularity globally due to its potential to reduce the reliance on traditional water sources and minimize energy consumption. In this article, we present a smartphone-based model for designing and optimizing RWHS with covered storage tanks, with the aim of providing a reliable and sustainable solution for water management.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"give is intro of Modeling Rainwater Harvesting Systems with Covered Storage Tank on A Smartphone\"\n",
    "response = get_llm_response(query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
